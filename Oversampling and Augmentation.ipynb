{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5c6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc58e2",
   "metadata": {},
   "source": [
    "## Load Data Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380c3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_excel(\"Data/data_train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af520074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Kalimat</th>\n",
       "      <th>Kalimat_prep</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>986</td>\n",
       "      <td>Esia yang terlupakan \\nGak kayak Smartfren sih...</td>\n",
       "      <td>esia yang terlupakan gak kayak smartfren sih y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2670</td>\n",
       "      <td>Kalau menurut saya, oknum KPAI yg cari panggun...</td>\n",
       "      <td>kalau menurut saya oknum kpai yang cari panggu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1169</td>\n",
       "      <td>KUHP .. Kasih Uang Habis Perkara</td>\n",
       "      <td>kuhp kasih uang habis perkara</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>880</td>\n",
       "      <td>Kok boleh ngerekam? Bukannya xxi ada camera in...</td>\n",
       "      <td>kok boleh ngerekam bukannya xxi ada camera inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2701</td>\n",
       "      <td>Djarum kudus lgsg menghentikan bingung juga kp...</td>\n",
       "      <td>djarum kudus langsung menghentikan bingung jug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            Kalimat  \\\n",
       "0    986  Esia yang terlupakan \\nGak kayak Smartfren sih...   \n",
       "1   2670  Kalau menurut saya, oknum KPAI yg cari panggun...   \n",
       "2   1169                   KUHP .. Kasih Uang Habis Perkara   \n",
       "3    880  Kok boleh ngerekam? Bukannya xxi ada camera in...   \n",
       "4   2701  Djarum kudus lgsg menghentikan bingung juga kp...   \n",
       "\n",
       "                                        Kalimat_prep  label  \n",
       "0  esia yang terlupakan gak kayak smartfren sih y...      1  \n",
       "1  kalau menurut saya oknum kpai yang cari panggu...      1  \n",
       "2                      kuhp kasih uang habis perkara      1  \n",
       "3  kok boleh ngerekam bukannya xxi ada camera inf...      1  \n",
       "4  djarum kudus langsung menghentikan bingung jug...      1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd4c4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414de554",
   "metadata": {},
   "source": [
    "## Create Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4188e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence) for sentence in data_train['Kalimat_prep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d467a5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3793241, 4424200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1)\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eafe6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Model/word2vec/w2v_train.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "0800b528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dikorupsi', 0.6762087941169739),\n",
       " ('berkarya', 0.6713862419128418),\n",
       " ('usemua', 0.6655163168907166),\n",
       " ('kurangi', 0.6282694339752197),\n",
       " ('medsosnya', 0.6166490316390991),\n",
       " ('setujuh', 0.6150714159011841),\n",
       " ('harumnya', 0.6131832003593445),\n",
       " ('terlanjur', 0.6129735708236694),\n",
       " ('pelatih', 0.6114039421081543),\n",
       " ('menggabungkan', 0.6039972305297852),\n",
       " ('pindahkan', 0.602449357509613),\n",
       " ('berjuamg', 0.602048397064209),\n",
       " ('mengesampingkan', 0.599072277545929),\n",
       " ('merek', 0.5954675674438477),\n",
       " ('membimbing', 0.5949916839599609),\n",
       " ('bermanfaatnya', 0.5945578217506409),\n",
       " ('mewujudkan', 0.5935664176940918),\n",
       " ('berbahaya', 0.593377947807312),\n",
       " ('block', 0.5928225517272949),\n",
       " ('makany', 0.5904486179351807)]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"benci\",topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63299b",
   "metadata": {},
   "source": [
    "## Easy Data Augmentation (EDA)\n",
    "https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py\n",
    "\n",
    "https://arxiv.org/pdf/1901.11196.pdf\n",
    "\n",
    "Karena bahasa Indonesia, wordnet diganti dengan word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0419629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f12373a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words list\n",
    "stop_words = [\"aku\",\"saya\",\"kamu\",\"anda\",\"kami\",\"kita\",\"kalian\",\"hai\",\n",
    "              \"yang\",\"di\",\"ke\",\"dari\",\"untuk\",\"sehingga\",\"karena\",\"oleh\",\"itu\",\n",
    "              \"maka\",\"makanya\",\"iya\",\"tidak\",\"gak\",\"dan\",\"tapi\",\"tetapi\",\"akan\",\n",
    "              '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0307d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"Model/word2vec/w2v_train.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "00daea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "369b7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            #print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: #only replace up to n words\n",
    "            break\n",
    "\n",
    "    #this is stupid but we need it, trust me\n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "    try:\n",
    "        synonyms = [i[0] for i in model.wv.most_similar(word,topn=20)]\n",
    "        return list(synonyms)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "    #obviously, if there's only one word, don't delete it\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    #randomly delete words with probability p\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    #if you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    \n",
    "    sentence = get_only_chars(sentence)\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word is not '']\n",
    "    num_words = len(words)\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    num_tech = int(sum([np.ceil(alpha_sr),np.ceil(alpha_ri),np.ceil(alpha_rs),np.ceil(p_rd)]))\n",
    "    num_new_per_technique = int(num_aug/num_tech)+1 if num_aug%num_tech!=0 else int(num_aug/num_tech)\n",
    "\n",
    "    #sr\n",
    "    if (alpha_sr > 0):\n",
    "        n_sr = max(1, int(alpha_sr*num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = synonym_replacement(words, n_sr)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #ri\n",
    "    if (alpha_ri > 0):\n",
    "        n_ri = max(1, int(alpha_ri*num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_insertion(words, n_ri)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #rs\n",
    "    if (alpha_rs > 0):\n",
    "        n_rs = max(1, int(alpha_rs*num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_swap(words, n_rs)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #rd\n",
    "    if (p_rd > 0):\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_deletion(words, p_rd)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "    shuffle(augmented_sentences)\n",
    "\n",
    "    #trim so that we have the desired number of augmented sentences\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "    #append the original sentence\n",
    "    # augmented_sentences.append(sentence)\n",
    "\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "b44e7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eda(sentence_ls, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.05, num_aug=8):\n",
    "    new_sentence_ls = []\n",
    "    for sent in tqdm(sentence_ls):\n",
    "        new_sentences = eda(sent,alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=p_rd,num_aug=num_aug)\n",
    "        new_sentence_ls = new_sentence_ls + new_sentences\n",
    "    return new_sentence_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc042f",
   "metadata": {},
   "source": [
    "## Oversampling EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ac73a41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_ovr = data_train.copy()\n",
    "data_train_ovr = data_train_ovr.drop([\"index\",\"Kalimat\"],axis=1)\n",
    "data_train_ovr['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5b33cbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 26, 3: 10}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_class = data_train_ovr['label'].value_counts()[1]\n",
    "num_aug_dict = {\n",
    "    2 : int(np.ceil(major_class/data_train_ovr['label'].value_counts()[2])),\n",
    "    3 : int(np.ceil(major_class/data_train_ovr['label'].value_counts()[3]))\n",
    "}\n",
    "num_aug_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5c2b583f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 91.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 219.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#oversampling label\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_ovr[data_train_ovr['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,num_aug=num_aug_dict[i])\n",
    "    max_new_sent = major_class - data_train_ovr['label'].value_counts()[i]#data_train_ovr['label'].value_counts()[i]*8 \n",
    "    data_train_ovr = pd.concat([data_train_ovr,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "eaf589ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3    1939\n",
       "2    1939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_ovr['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ca1f0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ovr.to_excel(\"Data/data_train_oversampling.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740b034",
   "metadata": {},
   "source": [
    "### SR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "de047cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2b844848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_sr = data_train.copy()\n",
    "data_train_sr = data_train_sr.drop([\"index\",\"Kalimat\"],axis=1)\n",
    "data_train_sr['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6e58f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 40.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 211/211 [00:02<00:00, 92.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#oversampling label\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_sr[data_train_sr['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,alpha_sr=0.05, alpha_ri=0, alpha_rs=0, p_rd=0,num_aug=num_aug_dict[i])\n",
    "    max_new_sent = major_class - data_train_sr['label'].value_counts()[i]\n",
    "    data_train_sr = pd.concat([data_train_sr,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9070aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3    1939\n",
       "2    1939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_sr['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "647a14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_sr.to_excel(\"Data/data_train_edasr.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e31fae",
   "metadata": {},
   "source": [
    "### RI Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e770930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e921606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_ri = data_train.copy()\n",
    "data_train_ri = data_train_ri.drop([\"index\",\"Kalimat\"],axis=1)\n",
    "data_train_ri['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fd3cfb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 41.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 211/211 [00:01<00:00, 107.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#oversampling label\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_ri[data_train_ri['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,alpha_sr=0, alpha_ri=0.05, alpha_rs=0, p_rd=0,num_aug=num_aug_dict[i])\n",
    "    max_new_sent = major_class - data_train_ri['label'].value_counts()[i]\n",
    "    data_train_ri = pd.concat([data_train_ri,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3bd1efbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3    1939\n",
       "2    1939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_ri['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fd53ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ri.to_excel(\"Data/data_train_edari.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237326a7",
   "metadata": {},
   "source": [
    "### RS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8c5bba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "628e3ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_rs = data_train.copy()\n",
    "data_train_rs = data_train_rs.drop([\"index\",\"Kalimat\"],axis=1)\n",
    "data_train_rs['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b07cb14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 1302.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 2178.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#oversampling label\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_rs[data_train_rs['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,alpha_sr=0, alpha_ri=0, alpha_rs=0.05, p_rd=0,num_aug=num_aug_dict[i])\n",
    "    max_new_sent = major_class - data_train_rs['label'].value_counts()[i]\n",
    "    data_train_rs = pd.concat([data_train_rs,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fc7d0d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3    1939\n",
       "2    1939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_rs['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "776a351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_rs.to_excel(\"Data/data_train_edars.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810723b4",
   "metadata": {},
   "source": [
    "### RD Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7af7eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "14af2a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_rd = data_train.copy()\n",
    "data_train_rd = data_train_rd.drop([\"index\",\"Kalimat\"],axis=1)\n",
    "data_train_rd['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dd77f153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 1093.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 2304.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#oversampling label\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_rd[data_train_rd['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,alpha_sr=0, alpha_ri=0, alpha_rs=0, p_rd=0.05,num_aug=num_aug_dict[i])\n",
    "    max_new_sent = major_class - data_train_rd['label'].value_counts()[i]\n",
    "    data_train_rd = pd.concat([data_train_rd,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d0d0c61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3    1939\n",
       "2    1939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_rd['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "420a2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_rd.to_excel(\"Data/data_train_edard.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7ddfa",
   "metadata": {},
   "source": [
    "### RI and RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9d6fb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "6e04e3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1939\n",
       "3     211\n",
       "2      75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_rird = data_train.copy()\n",
    "data_train_rird = data_train_rird.drop([\"index\",\"Kalimat\"],axis=1)\n",
    "data_train_rird['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5a33bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 86.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 227.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#oversampling label\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_rird[data_train_rird['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,alpha_sr=0, alpha_ri=0.05, alpha_rs=0, p_rd=0.05,num_aug=num_aug_dict[i])\n",
    "    max_new_sent = major_class - data_train_rird['label'].value_counts()[i]\n",
    "    data_train_rird = pd.concat([data_train_rird,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5855ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_rird.to_excel(\"Data/data_train_edarird.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abcb685",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9303eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f820067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ovr = pd.read_excel(\"Data/data_train_oversampling.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b26742f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1939\n",
       "2    1939\n",
       "1    1938\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_aug = data_train_ovr.copy()\n",
    "data_train_aug = data_train_aug[data_train_aug['Kalimat_prep']!=\"'\"]\n",
    "data_train_aug['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ccb50f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1938/1938 [00:06<00:00, 316.63it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1939/1939 [00:05<00:00, 338.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1939/1939 [00:06<00:00, 312.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#augmentation label\n",
    "for i in [1,2,3]:\n",
    "    sent_ls = data_train_aug[data_train_aug['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,num_aug=8)\n",
    "    max_new_sent = data_train_aug['label'].value_counts()[i]*8 \n",
    "    data_train_aug = pd.concat([data_train_aug,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                             'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "fca6d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_aug.to_excel(\"Data/data_train_augment.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3477b",
   "metadata": {},
   "source": [
    "## EDA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "09666a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "71c02271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    211\n",
       "2     75\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_eg = data_train.copy()\n",
    "data_train_eg = data_train_eg.drop([\"index\"],axis=1)\n",
    "data_train_eg = data_train_eg[data_train_eg['label'].isin([2,3])]\n",
    "data_train_eg['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "7b67bc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kalimat</th>\n",
       "      <th>Kalimat_prep</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kedunguan kpai dalam menegakkan uu... sampah!!!</td>\n",
       "      <td>kedunguan kpai dalam menegakkan uu sampah</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ga usah dikomenin kalo yang ini. Jenis-jenis m...</td>\n",
       "      <td>ga usah dikomenin kalo yang ini jenis jenis ma...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kpai .... pancen goblok</td>\n",
       "      <td>kpai pancen goblok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intinya cmn 1: demi ngeksis, cari perhatian. u...</td>\n",
       "      <td>intinya cuman demi ngeksis cari perhatian udah...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perusahaan rugi dibeli.. oooonnn koq dipiara y...</td>\n",
       "      <td>perusahaan rugi dibeli oon koq dipiara ya demi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Kalimat  \\\n",
       "0    kedunguan kpai dalam menegakkan uu... sampah!!!   \n",
       "1  Ga usah dikomenin kalo yang ini. Jenis-jenis m...   \n",
       "2                            kpai .... pancen goblok   \n",
       "3  intinya cmn 1: demi ngeksis, cari perhatian. u...   \n",
       "4  perusahaan rugi dibeli.. oooonnn koq dipiara y...   \n",
       "\n",
       "                                        Kalimat_prep  label  \n",
       "0          kedunguan kpai dalam menegakkan uu sampah      3  \n",
       "1  ga usah dikomenin kalo yang ini jenis jenis ma...      3  \n",
       "2                                 kpai pancen goblok      3  \n",
       "3  intinya cuman demi ngeksis cari perhatian udah...      2  \n",
       "4  perusahaan rugi dibeli oon koq dipiara ya demi...      3  "
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_eg = data_train_eg.sample(5,random_state=60).reset_index(drop=True)\n",
    "data_train_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e1297631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('menegakkan', 0.8644261956214905),\n",
       " ('selesai', 0.7761455178260803),\n",
       " ('pekan', 0.7281691431999207),\n",
       " ('targetkan', 0.7059224247932434),\n",
       " ('membuati', 0.6981822848320007),\n",
       " ('elemen', 0.6782402396202087),\n",
       " ('kecakapannya', 0.668877124786377),\n",
       " ('diundangkan', 0.6674259901046753),\n",
       " ('afiliasi', 0.6637691259384155),\n",
       " ('perubahan', 0.6615577936172485),\n",
       " ('disahkan', 0.6608225703239441),\n",
       " ('tameng', 0.6543505191802979),\n",
       " ('resmi', 0.6526904106140137),\n",
       " ('tiga', 0.6505138278007507),\n",
       " ('kakean', 0.6488603949546814),\n",
       " ('dievaluasi', 0.6433383822441101),\n",
       " ('rujukan', 0.6426711678504944),\n",
       " ('pintarlah', 0.6423753499984741),\n",
       " ('terjal', 0.6406939029693604),\n",
       " ('rok', 0.6405861973762512)]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"kedunguan\",topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4a8de617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('masarakat', 0.6190102100372314),\n",
       " ('mulutmu', 0.6125452518463135),\n",
       " ('gentlr', 0.608872652053833),\n",
       " ('nyindir', 0.5933667421340942),\n",
       " ('kedunguan', 0.5758902430534363),\n",
       " ('plin', 0.5740864872932434),\n",
       " ('dibuang', 0.5739468932151794),\n",
       " ('mukanya', 0.5628902316093445),\n",
       " ('lndonesia', 0.5627104043960571),\n",
       " ('bubarkankpi', 0.5578151345252991),\n",
       " ('betmanfaat', 0.5497796535491943),\n",
       " ('busuk', 0.5355801582336426),\n",
       " ('ngabalin', 0.5260036587715149),\n",
       " ('bau', 0.525847852230072),\n",
       " ('berisi', 0.518186092376709),\n",
       " ('gosip', 0.50596022605896),\n",
       " ('nirprestasi', 0.48198845982551575),\n",
       " ('seru', 0.4745867848396301),\n",
       " ('uy', 0.4695127010345459),\n",
       " ('rahim', 0.4652632772922516)]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"sampah\",topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "62bf823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kedunguan kpai dalam menegakkan uu sampah'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_eg['Kalimat_prep'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f509c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 496.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kedunguan dalam uu sampah', 'kedunguan kpai dalam kedunguan menegakkan uu sampah', 'kedunguan kpai dalam menegakkan bertntangan sampah', 'kedunguan kpai sampah menegakkan uu dalam']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 496.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ga usah dikomenin kalo yang ini jenis jenis mahluk yang kebodohannya minta kebodohannya dibikin viral', 'ga usah dikomenin kalo yang ini jenis jahat jenis mahluk yang sengaja minta kebodohannya dibikin viral', 'ga ini dikomenin kalo yang usah jenis jenis mahluk yang sengaja minta kebodohannya dibikin viral', 'ga usah dikomenin yang ini jenis jenis mahluk yang sengaja minta dibikin viral']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pancen kpai goblok', 'kpai pancen goblok', 'gantiin pancen goblok', 'kpai pancen ketuane goblok']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intinya cuman demi ngeksis cari perhatian udah itu aja ane kalau tiketnya film yang nge hype paling banter foto nonton doang sambil ngetag cewe ane udah itu aja gak sampai ngerekam segala kampungan menurut ane', 'intinya demi ngeksis cari perhatian ane kalau nonton film yang nge hype banter foto doang sambil cewe ane udah itu aja sampai ngerekam segala menurut', 'intinya cuman demi ngeksis cari perhatian udah itu aja ane kalau nonton film yang nge hype paling banter foto tiketnya doang nikmati ngetag cewe ane udah itu aja gak sampai ngerekam segala kampungan menurut ane', 'intinya cuman demi ngeksis cari perhatian udah itu aja ane kalau nonton film yang nge hype paling banter foto tiketnya doang sambil ngetag cewe ane udah itu aja gak fenomenal sampai ngerekam segala kampungan menurut ane']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 199.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perusahaan dibeli oon koq dipiara ya demi kemenangan jabatan doang yang kaga ngerti mah bener yang mah ini cuman sandiwara sinetron buat bikin pendukungnya akhirnya asset negara kembali negara yang rugi guoblokk bukan macam untung', 'perusahaan rugi dibeli oon koq dipiara ya demi kemenangan berkuasa doang yang kaga ngerti mah ngerasanya bener yang ngerti mah tau ini cuman sandiwara alias sinetron buat bikin pendukungnya terbuai akhirnya asset negara kembali asset negara yang rugi guoblokk menyuruh macam freeport yang untung', 'perusahaan rugi dibeli oon koq dipiara ya demi kemenangan jabatan doang yang kaga ngerti mah pendukungnya ngerasanya bener yang ngerti mah tau ini cuman sandiwara alias sinetron buat bikin pendukungnya terbuai akhirnya asset negara kembali asset negara yang rugi leher guoblokk bukan macam freeport yang untung', 'rugi rugi alias oon koq dipiara ya demi kemenangan jabatan doang yang kaga ngerti mah ngerasanya bener yang ngerti mah tau ini cuman sandiwara dibeli sinetron buat bikin pendukungnya terbuai akhirnya asset negara kembali asset negara yang perusahaan guoblokk bukan macam freeport yang untung']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in data_train_eg['Kalimat_prep'].values:\n",
    "    random.seed(10)\n",
    "    print(generate_eda([i],p_rd=0.2,num_aug=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2e59c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 709.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.70it/s]\n"
     ]
    }
   ],
   "source": [
    "#augmentation label\n",
    "data_train_eg_aug = pd.DataFrame()\n",
    "for i in [2,3]:\n",
    "    sent_ls = data_train_eg[data_train_eg['label']==i]['Kalimat_prep'].values.tolist()\n",
    "    new_sent_ls = generate_eda(sent_ls,num_aug=4)\n",
    "    max_new_sent = data_train_eg['label'].value_counts()[i]*4 \n",
    "    data_train_eg_aug = pd.concat([data_train_eg_aug,pd.DataFrame({'Kalimat_prep':new_sent_ls[:max_new_sent],\n",
    "                                                                   'label':np.ones(max_new_sent).astype(int)*i})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6a8314d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kalimat_prep</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yang gan rupiah kontol</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yang kontol rupiah paripurna</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keenakan yang kontol rupiah gan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yang kontol rupiah gan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>capek disikat ini orang cuma mau begoin rakyat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>capek deh ini orang cuma mau begoin excel raky...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>capek deh ini orang cuma mau aja</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>capek deh cuma orang ini mau begoin rakyat aja</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kalo mau apa murah tunggu ntar jagoan lu mimpi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kalo mau apa murah tunggu ntar jagoan lu mimpi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kalo mau apa murah tunggu ntar jagoan lu mimpi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kalo mau apa murah tunggu ntar jagoan lu mimpi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>guoblog</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jangankan guoblog</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sindiran</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>guoblog</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kpai dongo goblok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kpai goblok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kpai goblok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kpai mahluk</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Kalimat_prep  label\n",
       "0                              yang gan rupiah kontol      2\n",
       "1                        yang kontol rupiah paripurna      2\n",
       "2                     keenakan yang kontol rupiah gan      2\n",
       "3                              yang kontol rupiah gan      2\n",
       "4   capek disikat ini orang cuma mau begoin rakyat...      2\n",
       "5   capek deh ini orang cuma mau begoin excel raky...      2\n",
       "6                    capek deh ini orang cuma mau aja      2\n",
       "7      capek deh cuma orang ini mau begoin rakyat aja      2\n",
       "8   kalo mau apa murah tunggu ntar jagoan lu mimpi...      2\n",
       "9   kalo mau apa murah tunggu ntar jagoan lu mimpi...      2\n",
       "10  kalo mau apa murah tunggu ntar jagoan lu mimpi...      2\n",
       "11  kalo mau apa murah tunggu ntar jagoan lu mimpi...      2\n",
       "12                                            guoblog      2\n",
       "13                                  jangankan guoblog      2\n",
       "14                                           sindiran      2\n",
       "15                                            guoblog      2\n",
       "0                                   kpai dongo goblok      3\n",
       "1                                         kpai goblok      3\n",
       "2                                         kpai goblok      3\n",
       "3                                         kpai mahluk      3"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_eg_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c1bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
